{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.feature_selection import chi2\n",
    "import pyepsg\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tools\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>incident_id</th>\n",
       "      <th>date</th>\n",
       "      <th>state</th>\n",
       "      <th>city_or_county</th>\n",
       "      <th>address</th>\n",
       "      <th>n_killed</th>\n",
       "      <th>n_injured</th>\n",
       "      <th>incident_url</th>\n",
       "      <th>source_url</th>\n",
       "      <th>incident_url_fields_missing</th>\n",
       "      <th>...</th>\n",
       "      <th>participant_age</th>\n",
       "      <th>participant_age_group</th>\n",
       "      <th>participant_gender</th>\n",
       "      <th>participant_name</th>\n",
       "      <th>participant_relationship</th>\n",
       "      <th>participant_status</th>\n",
       "      <th>participant_type</th>\n",
       "      <th>sources</th>\n",
       "      <th>state_house_district</th>\n",
       "      <th>state_senate_district</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>461105</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>Mckeesport</td>\n",
       "      <td>1506 Versailles Avenue and Coursin Street</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>http://www.gunviolencearchive.org/incident/461105</td>\n",
       "      <td>http://www.post-gazette.com/local/south/2013/0...</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0::20</td>\n",
       "      <td>0::Adult 18+||1::Adult 18+||2::Adult 18+||3::A...</td>\n",
       "      <td>0::Male||1::Male||3::Male||4::Female</td>\n",
       "      <td>0::Julian Sims</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0::Arrested||1::Injured||2::Injured||3::Injure...</td>\n",
       "      <td>0::Victim||1::Victim||2::Victim||3::Victim||4:...</td>\n",
       "      <td>http://pittsburgh.cbslocal.com/2013/01/01/4-pe...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>460726</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>California</td>\n",
       "      <td>Hawthorne</td>\n",
       "      <td>13500 block of Cerise Avenue</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>http://www.gunviolencearchive.org/incident/460726</td>\n",
       "      <td>http://www.dailybulletin.com/article/zz/201301...</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0::20</td>\n",
       "      <td>0::Adult 18+||1::Adult 18+||2::Adult 18+||3::A...</td>\n",
       "      <td>0::Male</td>\n",
       "      <td>0::Bernard Gillis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0::Killed||1::Injured||2::Injured||3::Injured</td>\n",
       "      <td>0::Victim||1::Victim||2::Victim||3::Victim||4:...</td>\n",
       "      <td>http://losangeles.cbslocal.com/2013/01/01/man-...</td>\n",
       "      <td>62.0</td>\n",
       "      <td>35.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>478855</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>Ohio</td>\n",
       "      <td>Lorain</td>\n",
       "      <td>1776 East 28th Street</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>http://www.gunviolencearchive.org/incident/478855</td>\n",
       "      <td>http://chronicle.northcoastnow.com/2013/02/14/...</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0::25||1::31||2::33||3::34||4::33</td>\n",
       "      <td>0::Adult 18+||1::Adult 18+||2::Adult 18+||3::A...</td>\n",
       "      <td>0::Male||1::Male||2::Male||3::Male||4::Male</td>\n",
       "      <td>0::Damien Bell||1::Desmen Noble||2::Herman Sea...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0::Injured, Unharmed, Arrested||1::Unharmed, A...</td>\n",
       "      <td>0::Subject-Suspect||1::Subject-Suspect||2::Vic...</td>\n",
       "      <td>http://www.morningjournal.com/general-news/201...</td>\n",
       "      <td>56.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>478925</td>\n",
       "      <td>2013-01-05</td>\n",
       "      <td>Colorado</td>\n",
       "      <td>Aurora</td>\n",
       "      <td>16000 block of East Ithaca Place</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>http://www.gunviolencearchive.org/incident/478925</td>\n",
       "      <td>http://www.dailydemocrat.com/20130106/aurora-s...</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0::29||1::33||2::56||3::33</td>\n",
       "      <td>0::Adult 18+||1::Adult 18+||2::Adult 18+||3::A...</td>\n",
       "      <td>0::Female||1::Male||2::Male||3::Male</td>\n",
       "      <td>0::Stacie Philbrook||1::Christopher Ratliffe||...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0::Killed||1::Killed||2::Killed||3::Killed</td>\n",
       "      <td>0::Victim||1::Victim||2::Victim||3::Subject-Su...</td>\n",
       "      <td>http://denver.cbslocal.com/2013/01/06/officer-...</td>\n",
       "      <td>40.0</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>478959</td>\n",
       "      <td>2013-01-07</td>\n",
       "      <td>North Carolina</td>\n",
       "      <td>Greensboro</td>\n",
       "      <td>307 Mourning Dove Terrace</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>http://www.gunviolencearchive.org/incident/478959</td>\n",
       "      <td>http://www.journalnow.com/news/local/article_d...</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0::18||1::46||2::14||3::47</td>\n",
       "      <td>0::Adult 18+||1::Adult 18+||2::Teen 12-17||3::...</td>\n",
       "      <td>0::Female||1::Male||2::Male||3::Female</td>\n",
       "      <td>0::Danielle Imani Jameison||1::Maurice Eugene ...</td>\n",
       "      <td>3::Family</td>\n",
       "      <td>0::Injured||1::Injured||2::Killed||3::Killed</td>\n",
       "      <td>0::Victim||1::Victim||2::Victim||3::Subject-Su...</td>\n",
       "      <td>http://myfox8.com/2013/01/08/update-mother-sho...</td>\n",
       "      <td>62.0</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   incident_id        date           state city_or_county  \\\n",
       "0       461105  2013-01-01    Pennsylvania     Mckeesport   \n",
       "1       460726  2013-01-01      California      Hawthorne   \n",
       "2       478855  2013-01-01            Ohio         Lorain   \n",
       "3       478925  2013-01-05        Colorado         Aurora   \n",
       "4       478959  2013-01-07  North Carolina     Greensboro   \n",
       "\n",
       "                                     address  n_killed  n_injured  \\\n",
       "0  1506 Versailles Avenue and Coursin Street         0          4   \n",
       "1               13500 block of Cerise Avenue         1          3   \n",
       "2                      1776 East 28th Street         1          3   \n",
       "3           16000 block of East Ithaca Place         4          0   \n",
       "4                  307 Mourning Dove Terrace         2          2   \n",
       "\n",
       "                                        incident_url  \\\n",
       "0  http://www.gunviolencearchive.org/incident/461105   \n",
       "1  http://www.gunviolencearchive.org/incident/460726   \n",
       "2  http://www.gunviolencearchive.org/incident/478855   \n",
       "3  http://www.gunviolencearchive.org/incident/478925   \n",
       "4  http://www.gunviolencearchive.org/incident/478959   \n",
       "\n",
       "                                          source_url  \\\n",
       "0  http://www.post-gazette.com/local/south/2013/0...   \n",
       "1  http://www.dailybulletin.com/article/zz/201301...   \n",
       "2  http://chronicle.northcoastnow.com/2013/02/14/...   \n",
       "3  http://www.dailydemocrat.com/20130106/aurora-s...   \n",
       "4  http://www.journalnow.com/news/local/article_d...   \n",
       "\n",
       "   incident_url_fields_missing  ...                    participant_age  \\\n",
       "0                        False  ...                              0::20   \n",
       "1                        False  ...                              0::20   \n",
       "2                        False  ...  0::25||1::31||2::33||3::34||4::33   \n",
       "3                        False  ...         0::29||1::33||2::56||3::33   \n",
       "4                        False  ...         0::18||1::46||2::14||3::47   \n",
       "\n",
       "                               participant_age_group  \\\n",
       "0  0::Adult 18+||1::Adult 18+||2::Adult 18+||3::A...   \n",
       "1  0::Adult 18+||1::Adult 18+||2::Adult 18+||3::A...   \n",
       "2  0::Adult 18+||1::Adult 18+||2::Adult 18+||3::A...   \n",
       "3  0::Adult 18+||1::Adult 18+||2::Adult 18+||3::A...   \n",
       "4  0::Adult 18+||1::Adult 18+||2::Teen 12-17||3::...   \n",
       "\n",
       "                            participant_gender  \\\n",
       "0         0::Male||1::Male||3::Male||4::Female   \n",
       "1                                      0::Male   \n",
       "2  0::Male||1::Male||2::Male||3::Male||4::Male   \n",
       "3         0::Female||1::Male||2::Male||3::Male   \n",
       "4       0::Female||1::Male||2::Male||3::Female   \n",
       "\n",
       "                                    participant_name  \\\n",
       "0                                     0::Julian Sims   \n",
       "1                                  0::Bernard Gillis   \n",
       "2  0::Damien Bell||1::Desmen Noble||2::Herman Sea...   \n",
       "3  0::Stacie Philbrook||1::Christopher Ratliffe||...   \n",
       "4  0::Danielle Imani Jameison||1::Maurice Eugene ...   \n",
       "\n",
       "   participant_relationship  \\\n",
       "0                       NaN   \n",
       "1                       NaN   \n",
       "2                       NaN   \n",
       "3                       NaN   \n",
       "4                 3::Family   \n",
       "\n",
       "                                  participant_status  \\\n",
       "0  0::Arrested||1::Injured||2::Injured||3::Injure...   \n",
       "1      0::Killed||1::Injured||2::Injured||3::Injured   \n",
       "2  0::Injured, Unharmed, Arrested||1::Unharmed, A...   \n",
       "3         0::Killed||1::Killed||2::Killed||3::Killed   \n",
       "4       0::Injured||1::Injured||2::Killed||3::Killed   \n",
       "\n",
       "                                    participant_type  \\\n",
       "0  0::Victim||1::Victim||2::Victim||3::Victim||4:...   \n",
       "1  0::Victim||1::Victim||2::Victim||3::Victim||4:...   \n",
       "2  0::Subject-Suspect||1::Subject-Suspect||2::Vic...   \n",
       "3  0::Victim||1::Victim||2::Victim||3::Subject-Su...   \n",
       "4  0::Victim||1::Victim||2::Victim||3::Subject-Su...   \n",
       "\n",
       "                                             sources state_house_district  \\\n",
       "0  http://pittsburgh.cbslocal.com/2013/01/01/4-pe...                  NaN   \n",
       "1  http://losangeles.cbslocal.com/2013/01/01/man-...                 62.0   \n",
       "2  http://www.morningjournal.com/general-news/201...                 56.0   \n",
       "3  http://denver.cbslocal.com/2013/01/06/officer-...                 40.0   \n",
       "4  http://myfox8.com/2013/01/08/update-mother-sho...                 62.0   \n",
       "\n",
       "  state_senate_district  \n",
       "0                   NaN  \n",
       "1                  35.0  \n",
       "2                  13.0  \n",
       "3                  28.0  \n",
       "4                  27.0  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing data\n",
    "\n",
    "file = '/Users/tommy/documents/Database Design/Final_Proj/gun-violence-data_01-2013_03-2018.csv'\n",
    "\n",
    "shooting_data = pd.read_csv(file)\n",
    "\n",
    "shooting_data = shooting_data\n",
    "\n",
    "text_data = shooting_data['incident_characteristics'].map(str) + ' ' + shooting_data['notes'].map(str)\n",
    "\n",
    "death_data = shooting_data['n_killed']\n",
    "\n",
    "print(max(death_data))\n",
    "\n",
    "shooting_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning up data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing stopwords such as and, or, a, etc.\n",
    "\n",
    "Removing any non-letter elements (-,/,:, etc.)\n",
    "\n",
    "Creating a corpus to run TF-IDF on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(text_data):\n",
    "    \n",
    "    corpus = []\n",
    "    for i in range(len(text_data)):\n",
    "        text_data[i] = re.sub('[^a-zA-Z]', ' ', text_data[i])\n",
    "        text_data[i] = text_data[i].lower()\n",
    "        text_data[i] = text_data[i].split()\n",
    "        ps = PorterStemmer()\n",
    "        text_data[i] = [ps.stem(word) for word in text_data[i] if not word in set(stopwords.words('english'))]\n",
    "        text_data[i] = ' '.join(text_data[i])\n",
    "        corpus.append(text_data[i])\n",
    "    return text_data, corpus\n",
    "\n",
    "text_data, corpus = clean_data(text_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the Data to TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tf-idf stands for term frequency-inverse document frequency, and the tf-idf weight is a weight often used in information retrieval and text mining. This weight is a statistical measure used to evaluate how important a word is to a document in a collection or corpus. (http://www.tfidf.com/)\n",
    "\n",
    "TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document)\n",
    "\n",
    "IDF(t) = log_e(Total number of documents / Number of documents with term t in it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "tfidf.fit(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shot wound injur shot dead murder accident suicid mass shoot victim injur kill exclud subject suspect perpetr one locat gang involv four shot one kill unidentifi shooter getaway car\n"
     ]
    }
   ],
   "source": [
    "X = text_data\n",
    "y = death_data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_summary(pipeline, X_train, y_train, X_test, y_test):\n",
    "    sentiment_fit = pipeline.fit(X_train, y_train)\n",
    "    y_pred = sentiment_fit.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(\"accuracy score: {0:.2f}%\".format(accuracy*100))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is using SKLearn's random tree classifier to assist in determining Max Feature "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This block of code is determining the most accurate Max Feature variable to pass into the LSTM by using Random Forest, a simpler machine learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for trigram with stop words (Tfidf)\n",
      "\n",
      "RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
      "            criterion='gini', max_depth=None, max_features='auto',\n",
      "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "            min_impurity_split=None, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators='warn', n_jobs=None, oob_score=False,\n",
      "            random_state=None, verbose=0, warm_start=False)\n",
      "\n",
      "\n",
      "Test result for 10000 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score: 98.02%\n",
      "Test result for 15000 features\n",
      "accuracy score: 98.00%\n",
      "Test result for 20000 features\n",
      "accuracy score: 97.91%\n",
      "Test result for 25000 features\n",
      "accuracy score: 97.84%\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer()\n",
    "rf = RandomForestClassifier(class_weight=\"balanced\")\n",
    "n_features = np.arange(10000,25001,5000)\n",
    "def nfeature_accuracy_checker(vectorizer=cv, n_features=n_features, stop_words=None, ngram_range=(1, 1), classifier=rf):\n",
    "    result = []\n",
    "    print(classifier)\n",
    "    print(\"\\n\")\n",
    "    for n in n_features:\n",
    "        vectorizer.set_params(stop_words=stop_words, max_features=n, ngram_range=ngram_range)\n",
    "        checker_pipeline = Pipeline([\n",
    "            ('vectorizer', vectorizer),\n",
    "            ('classifier', classifier)\n",
    "        ])\n",
    "        print(\"Test result for {} features\".format(n))\n",
    "        nfeature_accuracy = accuracy_summary(checker_pipeline, X_train, y_train, X_test, y_test)\n",
    "        result.append((n,nfeature_accuracy))\n",
    "    return result\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "print(\"Result for trigram with stop words (Tfidf)\\n\")\n",
    "feature_result_tgt = nfeature_accuracy_checker(vectorizer=tfidf,ngram_range=(1, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating test sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/keras_preprocessing/text.py:178: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
      "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(179757, 107) (179757, 16)\n",
      "(59920, 107) (59920, 16)\n"
     ]
    }
   ],
   "source": [
    "#after finding the most accurate feature amount from the random forest classifier\n",
    "max_features = 15000\n",
    "#tokenizing text to numerical values (tokens)\n",
    "tokenizer = Tokenizer(nb_words=max_features, split=' ')\n",
    "tokenizer.fit_on_texts(text_data.values)\n",
    "#creating randomized text sequences from the text data\n",
    "X1 = tokenizer.texts_to_sequences(text_data.values)\n",
    "#padding the sequences so they are the same length\n",
    "X1 = pad_sequences(X1)\n",
    "Y1 = pd.get_dummies(death_data).values\n",
    "X1_train, X1_test, Y1_train, Y1_test = train_test_split(X1,Y1, random_state = 42)\n",
    "print(X1_train.shape,Y1_train.shape)\n",
    "print(X1_test.shape,Y1_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing the graph of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a 3 layer RNN with a LSTM hidden layer in the middle.\n",
    "\n",
    "This is a sequence to vector model, input tokenized sequence and returns a vector of probabilities and results.\n",
    "\n",
    "The first layer embeds the sequences into vectors that can be passed into the LSTM layer.\n",
    "\n",
    "The LSTM layer contains weights and biases like any Recursive Neural Network, however, is has a unique alpha between each node that determines \"the memory\" of each node. The sequences are viewed in a time-series fashion, where each proceeding word is weighted and then added to the result of the previously weighted words. Also an LSTM utilizes skip-connections in order to address the vanishing/exploding gradient problem.\n",
    "\n",
    "The final layer is the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:4: UserWarning: The `dropout` argument is no longer support in `Embedding`. You can apply a `keras.layers.SpatialDropout1D` layer right after the `Embedding` layer to get the same behavior.\n",
      "  after removing the cwd from sys.path.\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(200, dropout=0.2, recurrent_dropout=0.2)`\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 107, 150)          4500000   \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 200)               280800    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                3216      \n",
      "=================================================================\n",
      "Total params: 4,784,016\n",
      "Trainable params: 4,784,016\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 150\n",
    "lstm_out = 200\n",
    "model = Sequential()\n",
    "#X1 is padded so the input length is consistent for the entire structure\n",
    "model.add(Embedding(max_features, embed_dim,input_length = X1.shape[1]))\n",
    "model.add(LSTM(lstm_out, dropout_U=0.2,dropout_W=0.2))\n",
    "#final output layer of the RNN\n",
    "#16 is derived the max value from the number of deaths\n",
    "#in this case # of deaths is considered a class\n",
    "model.add(Dense(16,activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:2: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      " - 922s - loss: 0.0941 - acc: 0.9773\n",
      "Epoch 2/3\n",
      " - 948s - loss: 0.0685 - acc: 0.9832\n",
      "Epoch 3/3\n",
      " - 952s - loss: 0.0624 - acc: 0.9839\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x19a270550>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the batch size is on the lower end because the model converges pretty quickly\n",
    "#only needed 3 epochs because the accuracy began high and converged quickly\n",
    "\n",
    "batch_size = 32\n",
    "model.fit(X1_train, Y1_train, nb_epoch = 3, batch_size=batch_size, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.07\n",
      "acc: 0.98\n"
     ]
    }
   ],
   "source": [
    "score,acc = model.evaluate(X1_test, Y1_test, verbose = 2, batch_size = batch_size)\n",
    "print(\"score: %.2f\" % (score))\n",
    "print(\"acc: %.2f\" % (acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking the accuracy of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the model.predict function to pass in the test text (X1_test) and comparing the result to that of the test result in the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_acc 98.38618157543391 %\n"
     ]
    }
   ],
   "source": [
    "pos_cnt, pos_correct = 0, 0\n",
    "\n",
    "result_array, test_array = np.zeros(len(X1_test)), np.zeros(len(X1_test))\n",
    "\n",
    "for x in range(len(X1_test)):\n",
    "    \n",
    "    result = model.predict(X1_test[x].reshape(1,X1_test.shape[1]),batch_size=1,verbose = 2)[0]\n",
    "   \n",
    "    #saving prediction results vs. test results to create visualization\n",
    "    result_array[x] = np.argmax(result)\n",
    "    test_array[x] = np.argmax(Y1_test[x])\n",
    "    \n",
    "    if np.argmax(result) == np.argmax(Y1_test[x]):\n",
    "    \n",
    "            pos_correct += 1\n",
    "\n",
    "\n",
    "print(\"pos_acc\", pos_correct/len(X1_test)*100, \"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing the results of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The draw time for this plot will be slow for all clients.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~thomas_champ/2.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.empty(len(result_array))\n",
    "b = np.arange(0, len(result_array), 1)\n",
    "ind = np.arange(len(a))\n",
    "np.put(a, ind, b)\n",
    "\n",
    "tools.set_credentials_file(username='thomas_champ', api_key='fn1EyslX0oJ5MGRcWSpe')\n",
    "\n",
    "# Create a trace\n",
    "Model_Results = go.Scatter(\n",
    "    x = a,\n",
    "    y = result_array\n",
    ")\n",
    "\n",
    "Test_Results = go.Scatter(\n",
    "    x = a,\n",
    "    y = test_array\n",
    ")\n",
    "\n",
    "data = [Model_Results, Test_Results]\n",
    "\n",
    "py.iplot(data, filename='basic-scatter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load json and create model\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
